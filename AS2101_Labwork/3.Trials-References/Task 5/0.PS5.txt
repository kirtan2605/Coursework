Linear Least Squares using QR Decomposition

----------------------------------------------
for N data points, using linear regression, we have : 
Y = mX+c
i.e y = b0 + b1*x


which can be written in a matrix format:
A(nx2).x(2x1) = B(nx1)
where:
A =	[1  x1]
	[1  x2]
	[1  x3]
	[1  x4]
	[.. ..]
	[.. ..]
	[1  xn]
	
b =	[y1]    = [y1 y2 y3......yn]^T
	[y2]
	[y3]
	[y4]
	[..]
	[..]
	[yn]	
	
x =	[b0]    = [b0 b1]^T
	[b1]
	
To solve this, 
	((A^T).A).x = ((A^T).B)
taking  ((A^T).A) to be invertible, we solve for x using
	x = [((A^T).A)^(-1)].(A^T).B

m and c are obtained such that it "best fits" the data

The m and c obtained do not ensure that all the points are on the line.
	A.X - B = e
the best fit line obtained, minimizes this e

---------------------------------------------------------
Let us say the data function can be approximated using a function: y = b0 + b1*x + b2*(x^2)
Our goal is to find b0, b1, b2 for a 'least squares' solution.

We already know that we can re-write it as a matrix multiplication:
A(nx3).x(3x1) = B(nx1)
where:
A =	[ 1  x1  (x1)^2]
	[ 1  x2  (x2)^2]
	[ 1  x3  (x3)^2]
	[ 1  x4  (x4)^2]
	[..  ..  ......]
	[..  ..  ......]
	[ 1  xn  (xn)^2]
	
B =	[y1]  = [y1 y2 y3 . . . yn]^T
	[y2]
	[y3]
	[y4]
	[..]
	[..]
	[yn]	
	
x =	[b0]  = [b0 b1 b2]^T
	[b1]
	[b2]
	
which can be then written as A.x = B

in Task 2, we premultiplied by A^T and then found x. 
	(A^T).A.x = (A^T).B
	
in Task 5, we will solve this using QR decomposition.

The QR decomposition (also called the QR factorization) of a matrix is a decomposition of the matrix into an orthogonal matrix and a triangular matrix. A QR decomposition of a real square matrix A is a decomposition of A as
	A = QR,
where Q is an orthogonal matrix (i.e. Q^T.Q = I) and R is an upper triangular matrix.<If A is nonsingular, then this factorization is unique.>

There are several methods for actually computing the QR decomposition. One of such method is the Gram-Schmidt process.

then, we have A = Q.R, and (A^T) = (R^T).(Q^T)
After which, 
	(A^T).A.x = (A^T).B
	(R^T).(Q^T).Q.R.x= (R^T)(Q^T).B
in which (Q^T).Q = I
	(R^T).R.x = (R^T)(Q^T).B
	((R^T)^-1)(R^T).R.x = ((R^T)^-1)(R^T)(Q^T).B
	R.x = (Q^T).B
and since R is an Upper Triangular Matrix,
we can solve it directly using back substitution starting from last row and work up to the first row.

This method is more efficient because matrix inversion of dense matrices is a significantly expensive process.

The back-substitution makes it much faster.

Task5:
-Given a dataset of y vs x (let's say quadratic)
- find b0, b1, b2 using QR decomposition
	- use Gram-Schmidt process
	- use back-substituion
	- do not use any canned routines! (Write the entire process)
- compare against the process used in class 2
	- comment on accuracy and time taken
	
Furthermore, the Dataset given is NOT quadratic.
Check if y = b0 + b1*x + b2*(x^2) + b4*(x^4) is a better fit. Why or Why not.
------------------------------------------------------------
1. gs orthogonalization
2. QR decomposition
3. main
